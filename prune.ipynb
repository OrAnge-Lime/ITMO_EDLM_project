{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from cartoon_gan_origin.models.generator import Generator  \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from cartoon_gan_origin.utils.loss import ContentLoss, AdversialLoss\n",
    "from cartoon_gan_origin.utils.transforms import get_default_transforms, get_no_aug_transform\n",
    "from cartoon_gan_origin.utils.datasets import get_dataloader\n",
    "from cartoon_gan_origin.utils.transforms import get_pair_transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from cartoon_gan_origin.models.discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = 'cartoon_gan_origin/checkpoints/trained_netG.pth'\n",
    "IMG_SIZE = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = \"cpu\"\n",
    "netG_orig = Generator().to(DEVICE)\n",
    "netG_orig.eval()\n",
    "netG_orig.load_state_dict(torch.load(WEIGHTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def inv_normalize(img):\n",
    "    # Adding 0.1 to all normalization values since the model is trained (erroneously) without correct de-normalization\n",
    "    mean = torch.Tensor([0.485, 0.456, 0.406]).to(DEVICE)\n",
    "    std = torch.Tensor([0.229, 0.224, 0.225]).to(DEVICE)\n",
    "\n",
    "    img = img * std.view(1, 3, 1, 1) + mean.view(1, 3, 1, 1)\n",
    "    img = img.clamp(0, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "input_path = 'cartoon_gan_origin/data/trainA/world_0012.jpg'  \n",
    "image = Image.open(input_path).convert('RGB')\n",
    "trf = get_no_aug_transform()\n",
    "image_list = torch.from_numpy(trf(image).numpy()).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = netG_orig(image_list)\n",
    "\n",
    "generated_images = inv_normalize(generated_images)\n",
    "generated_image = generated_images[0].cpu()\n",
    "TF.to_pil_image(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured pruning (no CPU speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "def structured_pruning_safe(model, amount=0.2):\n",
    "    print(f\"Pruning {amount*100}%\")\n",
    "    \n",
    "    for module in model.down:\n",
    "        if isinstance(module, nn.Sequential):\n",
    "            if len(module) > 1 and isinstance(module[1], nn.Conv2d):\n",
    "                conv_layer = module[1]\n",
    "                prune.ln_structured(conv_layer, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "                prune.remove(conv_layer, 'weight')\n",
    "    res_blocks_container = model.res[0]\n",
    "    \n",
    "    for layer in res_blocks_container:\n",
    "        if hasattr(layer, 'block'):            \n",
    "            first_conv = layer.block[0][1]\n",
    "            prune.ln_structured(first_conv, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "            prune.remove(first_conv, 'weight')\n",
    "        \n",
    "structured_pruning_safe(netG, amount=0.05)\n",
    "print(f\"Zero weights: {torch.sum(netG.res[0][0].block[0][1].weight == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'cartoon_gan_origin/data/trainA/world_0012.jpg'  \n",
    "image = Image.open(input_path).convert('RGB')\n",
    "trf = get_no_aug_transform()\n",
    "image_list = torch.from_numpy(trf(image).numpy()).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = netG(image_list)\n",
    "\n",
    "generated_images = inv_normalize(generated_images)\n",
    "generated_image = generated_images[0].cpu()\n",
    "TF.to_pil_image(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency-aware structured pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "\n",
    "netG = Generator().to(DEVICE)\n",
    "netG.eval()\n",
    "netG.load_state_dict(torch.load(WEIGHTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "batch_size = 8\n",
    "image_size = 256\n",
    "\n",
    "# Dataloaders\n",
    "torch.manual_seed(1)\n",
    "real_dataloader    = get_dataloader(\"cartoon_gan_origin/data/trainA\", size = image_size, bs = batch_size)\n",
    "cartoon_dataloader = get_dataloader(\"cartoon_gan_origin/data/trainB_ghibli\", size = image_size, bs = batch_size, )\n",
    "edge_dataloader = get_dataloader(\"cartoon_gan_origin/data/trainB_ghibli\", size = image_size, bs = batch_size)\n",
    "\n",
    "last_epoch = 0\n",
    "last_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_images = next(iter(real_dataloader)).to(DEVICE)\n",
    "original_images = tracked_images.detach().cpu()\n",
    "grid = vutils.make_grid(original_images, padding=2, normalize=True, nrow=4)\n",
    "plt.imshow(np.transpose(grid, (1,2,0)).numpy())\n",
    "plt.imsave('cartoon_gan_origin/data/test/gt.jpg', np.transpose(grid, (1,2,0)).numpy())\n",
    "plt.figure()\n",
    "\n",
    "with torch.no_grad():\n",
    "    real = netG(tracked_images).detach()\n",
    "    real = inv_normalize(real).cpu()\n",
    "    \n",
    "grid = vutils.make_grid(real, padding=2, normalize=True, nrow=4)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "plt.imshow(np.transpose(grid, (1,2,0)).numpy())\n",
    "plt.imsave('cartoon_gan_origin/data/test/orig.jpg', np.transpose(grid, (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 5\n",
    "\n",
    "def prune_generator_modern(model, pruning_ratio=0.3):\n",
    "    example_inputs = torch.randn(1, 3, 256, 256)\n",
    "    imp = tp.importance.MagnitudeImportance(p=2)\n",
    "\n",
    "    ignored_layers = []\n",
    "    for name, module in netG.up[depth:].named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d):\n",
    "            ignored_layers.append(module)\n",
    "\n",
    "    for name, module in netG.down[:-depth].named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d):\n",
    "            ignored_layers.append(module)\n",
    "\n",
    "    pruner = tp.pruner.BasePruner(\n",
    "        model,\n",
    "        example_inputs,\n",
    "        importance=imp,\n",
    "        pruning_ratio=pruning_ratio,\n",
    "        ignored_layers=ignored_layers,\n",
    "        round_to=8, \n",
    "    )\n",
    "\n",
    "    base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(f\"Before Pruning: {base_nparams} params\")\n",
    "\n",
    "    pruner.step()\n",
    "\n",
    "    _, new_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(f\"After Pruning:  {new_nparams} params\")\n",
    "    print(f\"Reduction: {100 - (new_nparams/base_nparams)*100}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "netG.to(\"cpu\")\n",
    "netG = prune_generator_modern(netG, pruning_ratio=0.1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 3, 256, 256)\n",
    "    output = netG(dummy_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    # Should be [1, 3, 256, 256]\n",
    "\n",
    "netG = netG.to(DEVICE)\n",
    "\n",
    "input_path = 'cartoon_gan_origin/data/trainA/world_0012.jpg'  \n",
    "# input_path = 'cartoon_gan_origin/data/trainA/world_0032.jpg'  \n",
    "image = Image.open(input_path).convert('RGB')\n",
    "trf = get_no_aug_transform()\n",
    "image_list = torch.from_numpy(trf(image).numpy()).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = netG(image_list)\n",
    "\n",
    "generated_images = inv_normalize(generated_images)\n",
    "generated_image = generated_images[0].cpu()\n",
    "TF.to_pil_image(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model distilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "teacher_G = Generator().to(DEVICE)\n",
    "teacher_G.load_state_dict(torch.load(\"cartoon_gan_origin/checkpoints/trained_netG.pth\"))\n",
    "teacher_G.eval()\n",
    "for param in teacher_G.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "student_G = netG \n",
    "student_G.train()\n",
    "\n",
    "optimizerG = torch.optim.AdamW(student_G.parameters(), lr=1e-5, betas=(0.5, 0.999))\n",
    "\n",
    "distillation_criterion = nn.L1Loss().to(DEVICE)\n",
    "\n",
    "print(\"Starting Knowledge Distillation...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, real_data in enumerate(tqdm(real_dataloader)):\n",
    "        if i % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = student_G(tracked_images).detach()\n",
    "                \n",
    "            fake = inv_normalize(fake).cpu()\n",
    "            grid = vutils.make_grid(fake, padding=2, normalize=True, nrow=3)\n",
    "\n",
    "            os.makedirs(\"results\", exist_ok=True)\n",
    "            plt.imsave(f\"results/tune_ep_{epoch}_i{i}.png\", np.transpose(grid, (1,2,0)).numpy())\n",
    "            \n",
    "        real_data = real_data.to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_cartoon = teacher_G(real_data)\n",
    "            \n",
    "        predicted_cartoon = student_G(real_data)\n",
    "        \n",
    "        loss = distillation_criterion(predicted_cartoon, target_cartoon)\n",
    "        \n",
    "        optimizerG.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "             print(f\"Epoch {epoch} Iter {i} | Distillation Loss: {loss.item():.4f}\")\n",
    "\n",
    "best_model_state = copy.deepcopy(student_G.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'cartoon_gan_origin/data/test/ce2.jpg'\n",
    "# input_path = 'cartoon_gan_origin/data/trainA/world_0012.jpg'  \n",
    "# input_path = 'cartoon_gan_origin/data/trainA/world_0032.jpg'  \n",
    "image = Image.open(input_path).convert('RGB')\n",
    "trf = get_no_aug_transform()\n",
    "image_list = torch.from_numpy(trf(image).numpy()).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = netG(image_list)\n",
    "\n",
    "generated_images = inv_normalize(generated_images)\n",
    "generated_image = generated_images[0].cpu()\n",
    "TF.to_pil_image(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHTS1 = 'cartoon_gan_origin/checkpoints/prun_distilation_hard_G.pth'\n",
    "# netG = torch.load(WEIGHTS1, map_location='cpu', weights_only=False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fake = netG(tracked_images).detach()\n",
    "    fake = inv_normalize(fake).cpu()\n",
    "    \n",
    "grid = vutils.make_grid(tracked_images.cpu(), padding=2, normalize=True, nrow=4)\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load distil model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'cartoon_gan_origin/data/trainA/world_0012.jpg'  \n",
    "image = Image.open(input_path).convert('RGB')\n",
    "trf = get_no_aug_transform()\n",
    "image_list = torch.from_numpy(trf(image).numpy()).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = netG(image_list)\n",
    "\n",
    "generated_images = inv_normalize(generated_images)\n",
    "generated_image = generated_images[0].cpu()\n",
    "TF.to_pil_image(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG, \"cartoon_gan_origin/checkpoints/prun_distilation_hard_G.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_G = 1e-5  \n",
    "learning_rate_D = 1e-6 \n",
    "beta1, beta2 = (.5, .99)\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Models\n",
    "netD = Discriminator().to(DEVICE)\n",
    "netD.load_state_dict(torch.load(\"cartoon_gan_origin/checkpoints/trained_netD.pth\"))\n",
    "\n",
    "optimizerD = AdamW(netD.parameters(), lr=learning_rate_D, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "optimizerG = AdamW(netG.parameters(), lr=learning_rate_G, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "\n",
    "schedulerD = CyclicLR(optimizer=optimizerD, base_lr=learning_rate_D, max_lr=learning_rate_D*1e1, cycle_momentum=False)\n",
    "schedulerG = CyclicLR(optimizer=optimizerG, base_lr=learning_rate_G, max_lr=learning_rate_G*1e1, cycle_momentum=False)\n",
    "\n",
    "# Labels\n",
    "cartoon_labels = torch.ones(batch_size, 1, image_size // 4, image_size // 4).to(DEVICE)\n",
    "fake_labels    = torch.zeros(batch_size, 1, image_size // 4, image_size // 4).to(DEVICE)\n",
    "\n",
    "# Loss functions\n",
    "content_loss = ContentLoss(omega = 10).to(DEVICE)\n",
    "adv_loss     = AdversialLoss(cartoon_labels, fake_labels)\n",
    "BCE_loss     = nn.BCEWithLogitsLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "start_epoch = last_epoch\n",
    "start_i = last_i\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(start_epoch, epochs):    \n",
    "    print(f\"Epoch {epoch}\")\n",
    "    real_dl_iter = iter(real_dataloader)\n",
    "    cartoon_dl_iter = iter(cartoon_dataloader)\n",
    "    edge_dl_iter = iter(edge_dataloader)\n",
    "    iterations =  min(len(real_dl_iter), len(cartoon_dl_iter))\n",
    "    \n",
    "    for i in tqdm(range(start_i, iterations)):\n",
    "        real_data = next(real_dl_iter)\n",
    "        cartoon_data = next(cartoon_dl_iter)\n",
    "        edge_data = next(edge_dl_iter)\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "    \n",
    "        netD.train()\n",
    "        netG.eval()\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        cartoon_data   = cartoon_data.to(DEVICE)\n",
    "        edge_data      = edge_data.to(DEVICE)\n",
    "        real_data      = real_data.to(DEVICE)\n",
    "\n",
    "        generated_data = netG(real_data)\n",
    "\n",
    "        cartoon_pred   = netD(cartoon_data)      #.view(-1)\n",
    "        edge_pred      = netD(edge_data)         #.view(-1)\n",
    "        generated_pred = netD(generated_data)    #.view(-1)\n",
    "\n",
    "        errD = adv_loss(cartoon_pred, generated_pred, edge_pred)\n",
    "\n",
    "        errD.backward()\n",
    "        D_x = cartoon_pred.mean().item() # Should be close to 1\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.train()\n",
    "        netD.eval()\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        generated_data = netG(real_data)\n",
    "        generated_pred = netD(generated_data) #.view(-1)\n",
    "\n",
    "        # Calculate G's loss based on this output\n",
    "        cl = content_loss(generated_data, real_data)\n",
    "        bce = BCE_loss(generated_pred, cartoon_labels)\n",
    "        errG = bce + cl * 0.05\n",
    "        errG.backward()\n",
    "        D_G_z2 = generated_pred.mean().item() # Should be close to 1\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = netG(tracked_images).detach()\n",
    "                fake = inv_normalize(fake).cpu()\n",
    "                \n",
    "            grid = vutils.make_grid(fake, padding=2, normalize=True, nrow=3)\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "            os.makedirs(\"results\", exist_ok=True)\n",
    "            plt.imsave(f\"results/E{epoch}_i{i}_{time}.png\", np.transpose(grid, (1,2,0)).numpy())\n",
    "            img_list.append(grid)\n",
    "            \n",
    "            print(('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f\\t%s'\n",
    "                % (epoch, epochs, i, iterations, errD.item(), errG.item(), D_x, D_G_z2, time)).expandtabs(25))\n",
    "            \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        # D_losses.append(errD.item())\n",
    "        \n",
    "        # schedulerD.step()\n",
    "        schedulerG.step()\n",
    "        \n",
    "        last_i = i\n",
    "    start_i = 0\n",
    "    last_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG, \"cartoon_gan_origin/checkpoints/tune_trained_netG_05prun.pth\")\n",
    "torch.save(netD.state_dict(), \"cartoon_gan_origin/checkpoints/tune_trained_netD_05prun.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'cartoon_gan_origin/data/trainA/world_0012.jpg'  \n",
    "image = Image.open(input_path).convert('RGB')\n",
    "trf = get_no_aug_transform()\n",
    "image_list = torch.from_numpy(trf(image).numpy()).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = netG(image_list)\n",
    "\n",
    "generated_images = inv_normalize(generated_images)\n",
    "generated_image = generated_images[0].cpu()\n",
    "TF.to_pil_image(generated_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
